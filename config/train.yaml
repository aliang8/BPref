# defaults:
#     - agent: sac
#     - _self_

agent:
    # name: sac
    _target_: agent.sac.SACAgent
    obs_dim: ??? # to be specified later
    action_dim: ??? # to be specified later
    action_range: ??? # to be specified later
    device: ${device}
    critic_cfg: ${double_q_critic}
    actor_cfg: ${diag_gaussian_actor}
    discount: 0.99
    init_temperature: 0.1
    alpha_lr: 1e-4
    alpha_betas: [0.9, 0.999]
    actor_lr: 1e-4
    actor_betas: [0.9, 0.999]
    actor_update_frequency: 1
    critic_lr: 1e-4
    critic_betas: [0.9, 0.999]
    critic_tau: 0.005
    critic_target_update_frequency: 2
    batch_size: 512 # 1024 for Walker, 512 for Meta-world
    learnable_temperature: true
    
double_q_critic:
    _target_: agent.critic.DoubleQCritic
    obs_dim: ${agent.obs_dim}
    action_dim: ${agent.action_dim}
    hidden_dim: 256
    hidden_depth: 3
    
diag_gaussian_actor:
    _target_: agent.actor.DiagGaussianActor
    obs_dim: ${agent.obs_dim}
    action_dim: ${agent.action_dim}
    hidden_depth: 3
    hidden_dim: 256
    log_std_bounds: [-5, 2]

#env
env: dog_stand

# Basic setup
experiment: sac
device: cuda
seed: 1

# training 
num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}
num_seed_steps: 5000
eval_frequency: 10000
num_eval_episodes: 10

# unsupervise
num_unsup_steps: 0
topK: 5
reset_update: 100

# logger
log_frequency: 10000
log_save_tb: true

# video recorder
save_video: false
save_model: true

# hydra configuration
hydra:
    job:
        name: ${env}
    run:
        dir: ./exp/${env}/H${diag_gaussian_actor.hidden_dim}_L${diag_gaussian_actor.hidden_depth}_B${agent.batch_size}_tau${agent.critic_tau}/unsup${num_unsup_steps}_topk${topK}_${experiment}_lr${agent.actor_lr}_temp${agent.init_temperature}_seed${seed}
